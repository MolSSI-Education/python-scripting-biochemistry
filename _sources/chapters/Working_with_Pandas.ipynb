{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Pandas\n",
    "================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``````{admonition} Overview\n",
    ":class: overview\n",
    "\n",
    "Questions:\n",
    "\n",
    "* How can I use import data for analysis in my notebook?\n",
    "\n",
    "Objectives:\n",
    "\n",
    "* Import the pandas library.\n",
    "\n",
    "* Use pandas library funtions to import data from a csv file.\n",
    "\n",
    "* Import data from a .csv formatted file.\n",
    "\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is pandas and why do we use it?\n",
    "\n",
    "Pandas is a python library that is designed to work with two dimensional data arrays. It is built on numpy, another python library that specializes in numerical analysis. Numpy also has the ability to create and analyze data arrays. If you are not familiar with arrays, here are some simple examples.\n",
    "\n",
    "### 1D arrays\n",
    "A one dimensional array is simply a list of items, for example, a list of the elements: H, He, Li, Be, B, etc.\n",
    "\n",
    "### 2D arrays\n",
    "A two dimensional array is an array which has rows and columns. It can have any number of columns and rows. A spreadsheet with rows and columns is analogous to a 2D array.\n",
    "\n",
    "### 3D arrays\n",
    "A three-dimensional array would be a collection of two dimensional arrays. For example, this might be a collection of x, y, and z coordinates for a structure as a function of time. \n",
    "\n",
    "The numpy library has functions that will manage n-dimensional arrays, while pandas only works on 2D arrays. You may need numpy for nD arrays at some point, but for this workshop, we will learn to use pandas as we learn how to perform linear and nonlinear regression of laboratory data based on data in 2D arrays.\n",
    "\n",
    "The pandas library contains powerful tools for working with 2D data arrays, including the ability to identify the rows and columns of data by unique identifiers: things like \"Protein Concentration (mg/mL)\" and \"initial velocity\" rather than \"row 1\" or \"column C\"). Pandas also has many more functions that we will not explore in this workshop, but here are three excellent free online resources for learning more about pandas:\n",
    "\n",
    "+ [Using pandas for data analysis](https://education.molssi.org/python-data-analysis/02-pandas/index.html) from MolSSI\n",
    "+ Charlie Weiss's excellent online textbook, *Scientific Computing for Chemists*, which you can find on his GitHub site, [SciCompforChemists](https://weisscharlesj.github.io/SciCompforChemists/intro.html)\n",
    "+ Corey Schafer's [Pandas Tutorials on YouTube](https://www.youtube.com/watch?v=ZyhVh-qRZPA&list=PL-osiE80TeTsWmV9i9c58mdDCSskIFdDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing python libraries\n",
    "\n",
    "As we move toward data analysis, we will need to import a series of python **libraries**. A **library** is a set of modules which contain related functions that can be used to complete specific tasks. Using libraries in Python reduces the amount of code you have to write. Usually a function has some type of input and gives a particular output.  To use a function that is in a library, you often use the dot notation introduced previously with commands like `os.getcwd()`.\n",
    "\n",
    "In the file parsing lesson, we imported the `os` library, which can be used to assign the location of a file to a variable (e.g. datafile), so our script can be transferred between computers with different operating systems (e.g., Mac vs Windows). In this lesson, we will be using the `numpy` and `pandas`, and we will add a few more libraries in the future. As we proceed, we'll build this table. \n",
    "\n",
    "| Library | Uses | Abbreviation |\n",
    "| :------- | :----: | :------------: |\n",
    "| os | file management in operating systems | os |\n",
    "| numpy | calculations | np  | \n",
    "| pandas | data management | pd |\n",
    "\n",
    "To start we will import the two libraries we need to complete the work in this notebook, os and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with data in Jupyter notebooks, you must first find the data. In the file parsing lesson, we used the `os.getcwd()` (get current working directory) command to find out where we were located on our computers. We also used the `os.listdir()` command to see the contents of the directory. Let's do that to find our target datafile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir() # lists the contents of the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the lesson on file parsing, the list includes specific files ending with suffixes such as .ipynb, .png, and .zip. It also includes directories, such as images and data that contain additional files. To find the contents of the data directory, you simply enclose data in single quotes and add it to the `os.listdir()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice a number of files and folders in the data folder. We will be working with the file, *thrombin_with_ligands.csv*. As a first step, we will assign the location of that file to a variable, using the `path.join` function from the `os` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrombin_file = os.path.join('data', 'thrombin_with_ligands.csv')\n",
    "print(thrombin_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data with pandas\n",
    "\n",
    "As mentioned above, the pandas library has a number of excellent tools for importing data. We will be using the `read_csv` function from the pandas library to import our data. You may have encountered csv files before; csv stands for comma separated values. This is a format that spreadsheet programs like Excel and Google Sheets can both read and write. If you open a csv file in a text editor, it simply contains rows of information, where the information is separated by commas. The information can be integers, real numbers or strings. Here are the first four rows of the csv file that we will be exploring using pandas.\n",
    "\n",
    "```\n",
    "PDB ID,Method,Resolution,Structure,Ligand ID,Ligand name\n",
    "\n",
    "3SHC,X-RAY DIFFRACTION,1.9,Human Thrombin,B01,D-phenylalanyl-N-[(4-chloropyridin-2-yl)methyl]-L-prolinamide\n",
    "\n",
    "3P17,X-RAY DIFFRACTION,1.43,Thrombin,99P,D-phenylalanyl-N-(pyridin-3-ylmethyl)-L-prolinamide\n",
    "\n",
    "2ZNK,X-RAY DIFFRACTION,1.8,Thrombin,31U,D-leucyl-N-(4-carbamimidoylbenzyl)-L-prolinamide\n",
    "```\n",
    "\n",
    "If you look at these data carefully, you can see how they could easily be arranged in a table, with the labels in the first row as the column headers. Before we can explore how pandas can do that for us, let's look at the help command for the `read_csv` function in pandas.\n",
    "\n",
    "```python\n",
    "help(pd.read_csv)\n",
    "```\n",
    "\n",
    "Notice the syntax for the command:  help is followed by parentheses, which contain the function name that we want to explore. Also note that `pandas` is abbreviated as `pd`. That may seem strange at first, but you will get used to library abbreviations in short order if you work much with Jupyter notebooks, or with python in general.\n",
    "\n",
    "If you enter this command in a cell, the output will be over 200 lines long. You can learn about all of the arguments that can be attached to the read_csv function and how they affect the dataframe. Just to give you a taste, here are the first few lines you would see.\n",
    "\n",
    "Help on function read_csv in module pandas.io.parsers:\n",
    "\n",
    "```python\n",
    "read_csv(filepath_or_buffer: Union[ForwardRef('PathLike[str]'), str, IO[~T], io.RawIOBase, io.BufferedIOBase, io.TextIOBase, _io.TextIOWrapper, mmap.mmap], sep=<object object at 0x7f99809963e0>, delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal: str = '.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, dialect=None, error_bad_lines=True, warn_bad_lines=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None, storage_options: Union[Dict[str, Any], NoneType] = None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The panda library contains many more options than we have time to explore right now, but one thing to pay special attention to is called a `dataframe`. The [pandas dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) is the primary data structure that is produced and manipulated by pandas functions. We can take an existing 2D data array and tell pandas to convert it to a DataFrame, or we can use the read_csv command to convert a csv file to a pandas dataframe. As our example, we will be exploring a csv file containing information about different thrombin structures found in the [Protein Data Bank](https://www.rcsb.org/). So let's create our DataFrame and explore what it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrombin_df = pd.read_csv(thrombin_file) #thrombin_df is a convenient name for our dataframe.\n",
    "thrombin_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding information in a dataframe\n",
    "\n",
    "The dataframe appears as a table. The data are in rows (\"indexes\") and columns (\"series\"). If you want to look at the data in a single column (one series), you can use this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrombin_df['Resolution']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``````{admonition} Check your understanding\n",
    ":class: exercise\n",
    "\n",
    "How can you print out the ligand names for each of the structures?\n",
    "    \n",
    "```{admonition} Solution\n",
    ":class: solution dropdown\n",
    "\n",
    "```python\n",
    "thrombin_df['Ligand name']\n",
    "```\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the loc function from the pandas library to get all the information from a single index or row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrombin_df.loc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the loc command to locate the information from a specific cell. If you enter the index (row) of 2 and the column 'Resolution', the resolution for the structure in the third row is returned. You may be thinking, \"Resolution is the third column. Why does python think it is column 2?\" Remember that python starts counting at 0. So the number for the first column is 0 and the number for the third column is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrombin_df.loc[2, 'Resolution']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are times when you may want to use the index number and column number to locate the information in that cell in the dataframe. In this case, you can use the iloc (for integer locate) command, which accepts integers in the form of \\[row, column]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrombin_df.iloc[2,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to look at the first five rows of a dataframe, you can use the `head` function; you can use the `tail` function to look at the last five rows. We always include () after the name of the function. This is how we can pass arguments into the function. Try using the argument 3 in the head command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrombin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrombin_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thrombin_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting data with pandas\n",
    "You can also sort the data using the pandas sort_values function. To sort the thrombin_df dataframe into ascending Resolution values, use the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thrombin_df.sort_values('Resolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the numbers in the first column (the index values) are not in order. If you type “thrombin_df” now, you will get back the unsorted dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrombin_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you want to retain the sorting in your dataframe, you can add `inplace = True` to the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrombin_df.sort_values('Resolution', inplace = True)\n",
    "thrombin_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``````{admonition} Exercise\n",
    ":class: exercise\n",
    "\n",
    " The enzyme kinetics data from the introduction can be found in the file `enzyme_kinetics.csv` in the data directory. The only problem is that the substrate concentrations are out of order. This is how it looks in Excel:\n",
    "\n",
    "     \n",
    "![Enzyme Kinetics](images/enzyme_kinetics.png)\n",
    "\n",
    "Create a dataframe from that file and use pandas functions to place the data in ascending substrate concentrations, then print the velocity at a substrate concentration of 20 micromolar. As a biochemist, you probably want to plot and analyze these data to find the Km and Vmax. We'll get to that in a bit. The goal right now is to just have a little fun with pandas.\n",
    "    \n",
    "```{admonition} Solution\n",
    ":class: solution dropdown\n",
    "\n",
    "```python\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    kinetics_file = os.path.join('data', 'enzyme_kinetics.csv')\n",
    "    kinetics_df = pd.read_csv(kinetics_file) # kinetics_file is a variable, not a string\n",
    "    kinetics_df.head(0)  # to find out the column labels. You can also use kinetics_df.columns to get the same output.\n",
    "    kinetics_df.sort_values('Substrate_conc', inplace = True)  # once you know the column labels\n",
    "    kinetics_df.loc[9]\n",
    "```\n",
    "``````\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``````{admonition} Key Points\n",
    ":class: key\n",
    "\n",
    "* The pandas library can be used to analyze tabular (rows and columns) data.\n",
    "\n",
    "* The pandas library can read CSV (comma separated vaue) files and Excel files.\n",
    "\n",
    "``````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
